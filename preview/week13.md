### **1. Lecture's Agenda ğŸ**

- **Reinforce Learning**

### **2. Several Note-takings about the Lecture(ë‚´ìš© ì •ë¦¬) ğŸ§™**

- **Policy Gradient**
   ì•ì—ì„œ policy parameterization ë°©ë²•ì„ ìœ„í•œ performanceë¥¼ ì •ì˜í•˜ë©´ì„œ true value function v_{\pi_{\boldsymbol{\theta}}}ì´ ì‚¬ìš©ë˜ì—ˆë‹¤. ë¬¸ì œëŠ” performanceê°€ actionê³¼ states ëª¨ë‘ì˜ í•¨ìˆ˜ë¼ëŠ” ì ì´ë©°, ì´ ë‘˜ì€ policy weightì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. stateê°€ ì£¼ì–´ì§€ë©´, policy parameterizationì •ë³´ë¡œ ë¶€í„° action(ê²°êµ­, reward)ì— ë¯¸ì¹˜ëŠ” policy weightsì˜ ì˜í–¥ì„ ë¹„êµì  ëª…í™•í•˜ê²Œ ê³„ì‚°ë  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜, policyê°€ state distributionì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ì˜¨ì „íˆ environmentì˜ í•¨ìˆ˜ì´ë©°, ì•Œ ìˆ˜ ì—†ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ì–´ë–»ê²Œ policyê°€ ì˜í–¥ë°›ëŠ” state distributionì„ ëª¨ë¥¸ ìƒíƒœì—ì„œ ì´ ëª¨ë¥´ëŠ” ì •ë³´ì— ì˜í–¥ë°›ëŠ” policy weightsì˜ performance gradientë¥¼ ì¶”ì •í•  ìˆ˜ ìˆì„ê¹Œ? (ê¸€ì´ ë„ˆë¬´ ë³µì¡í•˜ë‹¤. ìš”ì ì€ performanceëŠ” policy weightsì— ì˜í–¥ì„ ë°›ëŠ” stateì™€ actionì˜ í•¨ìˆ˜ì´ì§€ë§Œ, ì•Œ ìˆ˜ ì—†ëŠ” stateì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ gradientë¥¼ êµ¬í•  ìˆ˜ ì—†ë‹¤ëŠ” ì–˜ê¸°ë‹¤.)
   
### **3. Summary(ì•Œê²Œ ëœ ë‚´ìš© ìš”ì•½) ğŸ§ **
- Policy Gradient
- Reinforce Algorithm

