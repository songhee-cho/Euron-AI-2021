### **0. Before the Class**

- History of CNN
  - 1957, perceptron: weight W update rule â†’ w<sub>i</sub>(t+1) = w<sub>i</sub>(t) + Î±(d<sub>j</sub> - y<sub>j</sub>(t))x<sub>j,i
  - 1960, multilayer perceptron network
  - 1986, backpropagation: âˆ‚E<sub>p</sub>/âˆ‚w<sub>ji</sub> = âˆ‚E<sub>p</sub>/âˆ‚o<sub>pj</sub> * âˆ‚o<sub>pj</sub>/âˆ‚w<sub>ji</sub>
  - 2006, deep learning
  - 2012, AlexNet â†’ CNN, batch normalization
  
### **1. Lecture's Agenda ğŸ**

- **Architecture of CNN**
- **Layers of CNN**
- **Structure of CNN**

### **2. Several Note-takings about the Lecture(ë‚´ìš© ì •ë¦¬) ğŸ§™**

- **Architecture of CNN:** computation graph â† chain rule
    - fully connected layer(FC layer/Dense Layer/Flatten Layer): 32Ã—32Ã—3 â†’ 3072Ã—1(input) â‡’ Wx â‡’ activation layer: 1Ã—10
    - convolutional layer: 32Ã—32Ã—3(input)â‡’ 5Ã—5Ã—3 filter: spatially dot product
      - filter slides over the input image(convolve) several times: w<sup>T</sup>x + b
      - activation map
      - the number of filters, the size of filters, stride
    
- **Layers of CNN:** 
  - convolutional layer
    - local connectivity
    - spatial arrangement
    - parameter sharing
  - pooling layer: commonly use maxpooling
    - to reduce the number of parameters(computation)
  - fully-connected layer
  
- **Structure of CNN**
  - INPUT ->  ((CONV -> RELU) x N -> POOL?)x M -> (FC -> RELU) x K -> FC
    - POOL?: optional
    - K activations
    - commonly 0<=N<3, M>=0, 0<=K<3

### **3. Summary(ì•Œê²Œ ëœ ë‚´ìš© ìš”ì•½) ğŸ§ **
- ConvNets stack CONV, POOL, FC layers
- Trend: smaller filers, deeper architectures, getting rid of POOL/FC
- ResNet, GoogLeNet
