### **0. Before the Class**

### **1. Lecture's Agenda ğŸ**: CNN Architectures

- **Case Studies**
  - AlexNet
  - VGG
  - GoogLeNet
  - ResNet
  - NiN(network in network)
  - Wide ResNet
  - ResNeXT
  - Stochastic Depth
  - DenseNet
  - FractalNet
  - SqueezeNet   

### **2. Several Note-takings about the Lecture(ë‚´ìš© ì •ë¦¬) ğŸ§™**

- **AlexNet** 
  - Conv layer â†’ Max pooling â†’ normalization
  - CONV1 â†’ MAX POOL1 â†’ NORM1 â†’ CONV2 â†’ MAX POOL2 â†’ NORM2 â†’ CONV3 â†’ CONV4 â†’ CONV5 â†’ MAX POOL3 â†’ FC6 â†’ FC7 â†’ FC8
  - output size = (input size - filterSize)/stride + 1
  - what is the # of parameters in max pooling layer?: # of parameters = 0
  - details of *AlexNet*
    - first use of ReLU
    - used Norm layers(not common anymore)
    - heavy data augmentation
    - dropout 0.5
    - batch size 128
    - SGD momentum 0.9
    - learning rate 1e-2, reduced by 10 manually when val accuracy plateaus
    - L2 weight decay 5e-4
    - 7 CNN ensemble

- **VGGNet**
  - small filters, deeper networks
  - why use smaller filters? (3Ã—3 conv)
    - stack of three 3Ã—3 conv(stride 1) layers has same effective receptive field as one 7Ã—7 conv layer
  - most memory is in early CONV
  - most params are in late FC

- **GoogLeNet**
  - 22 layers
  - efficient "inception" module
    - inception module: design a good local network topology(net within net) and then stack these modules on top of each other
  - No FC layers
  - Only 5 million params: 12x less than AlexNet 
    
### **3. Summary(ì•Œê²Œ ëœ ë‚´ìš© ìš”ì•½) ğŸ§ **
  - AlexNet
  - VGG
  - GoogLeNet
  - ResNet
  - NiN(network in network)
  - Wide ResNet
  - ResNeXT
  - Stochastic Depth
  - DenseNet
  - FractalNet
  - SqueezeNet   
