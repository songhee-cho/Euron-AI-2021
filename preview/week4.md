### **0. Before the Class**

- we define
    - score function: s = f(x;W) = Wx
    - SVM loss: L_i = Î£ jâ‰ y_i max(0, s_j - s_y_i + 1)
    - data loss + regularization:  L = 1/N Î£ i=1~N (L_i) + Î£k ((W_k)^2)

    â†’ still want to minimize loss function : gradient of L with respect to W = âˆ‡wL â†’ optimization

    - optimization
        - gradient descent: numerical gradient, analytic gradient

### **1. Lecture's Agenda ğŸ**

- **Backpropagation**
- **Gradients for vectorized code**
- **Neural Network**
- **Neuron and Neural Network**

### **2. Several Note-takings about the Lecture(ë‚´ìš© ì •ë¦¬) ğŸ§™**

- **Backpropagation:** computation graph â† chain rule
    - we want: âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y, âˆ‚f/âˆ‚xz
        - âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚q * âˆ‚q/âˆ‚x
        - local gradient: âˆ‚z/âˆ‚x, âˆ‚z/âˆ‚y
        - gradient: âˆ‚L/âˆ‚z
        - gradient = local gradient Ã— upstream gradient

- **Gradients for vectorized code:** Jacobian matrix âˆ‚f/âˆ‚x
  - Jacobian matrix: expression of derivatives in multivariable function
  - size of Jacobian matrix = (input size)Ã—(input size)
  - size of gradient = size of variable : they should have the same shape with each other

- **Neural Network**
  - linear score function: f = Wx 
  - 2-layer neural network: f = W2(max(0,W1x))
  - 3-layer neural network: f = W3(max(0,W2(max(0,W1x))))

- **Neuron and Neural Network**
  - axon form a neuron: x0
  - synapse: Ã— w0
  - dendrite: w0x0
  - cell body: Î£ wixi + b
  - activation function: f
  - output axon: f(Î£ wixi + b)
  - but acutally, biological neuron and nerual network are different 
  
### **3. Summary(ì•Œê²Œ ëœ ë‚´ìš© ìš”ì•½) ğŸ§ **
- backprop uses chain rule
- brain ananlogy of neural network is just conceptual one, and neural net is actually different from biological brain network.
